{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea404e77-eebe-4797-a05b-4dc1b6ebdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fd1987-d56d-4c16-bc12-3e1036c579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b333da1d-c0fa-4aff-b094-2a4de28727ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/corpora/stopwords/english','r') as file:\n",
    "    sw=file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170a07b2-0e75-44cb-9db1-b1de1651f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac2547c-117b-49ae-bd34-b1f7c3c5a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    data = pd.DataFrame([sentence], columns=['Text'])\n",
    "    data.loc[ :,\"Text\"] = data[\"Text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    data[\"Text\"]=data[\"Text\"].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x,flags=re.MULTILINE) for x in x.split()))\n",
    "    data[\"Text\"]=data[\"Text\"].apply(remove_punctuations)\n",
    "    data[\"Text\"] = data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "    data[\"Text\"]=data[\"Text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "    return data[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b0d9e9-581a-4aca-85c4-1f54700f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'great product. i like it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94e66e9-96fb-4951-9619-d9e5080d8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentence = preprocessing(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef269b4-e902-4b15-8396-6a631ee21211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    great product like\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "848a1c1b-49f4-4a08-b429-3b1181a911db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('../static/model/vocabulary.txt', header=None)\n",
    "vocab_tokens = vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a375a4-9e29-4ce0-9fbc-3386a0012266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cant',\n",
       " 'believ',\n",
       " 'first',\n",
       " 'review',\n",
       " 'great',\n",
       " 'productbr',\n",
       " 'br',\n",
       " 'use',\n",
       " 'best',\n",
       " 'organ',\n",
       " 'infant',\n",
       " 'formula',\n",
       " 'iron',\n",
       " 'ounc',\n",
       " 'also',\n",
       " 'product',\n",
       " 'littl',\n",
       " 'start',\n",
       " 'show',\n",
       " 'sign',\n",
       " 'lactos',\n",
       " 'intoler',\n",
       " 'pediatrician',\n",
       " 'search',\n",
       " 'soy',\n",
       " 'offer',\n",
       " 'free',\n",
       " 'altern',\n",
       " 'mani',\n",
       " 'prefer',\n",
       " 'want',\n",
       " 'natur',\n",
       " 'nasti',\n",
       " 'chemic',\n",
       " 'ingredi',\n",
       " 'consider',\n",
       " 'amount',\n",
       " 'research',\n",
       " 'shop',\n",
       " 'around',\n",
       " 'found',\n",
       " 'choic',\n",
       " 'market',\n",
       " 'one',\n",
       " 'took',\n",
       " 'well',\n",
       " 'lot',\n",
       " 'better',\n",
       " 'find',\n",
       " 'pack',\n",
       " 'dollar',\n",
       " 'less',\n",
       " 'reason',\n",
       " 'charg',\n",
       " 'much',\n",
       " 'month',\n",
       " 'old',\n",
       " 'origin',\n",
       " 'spit',\n",
       " 'st',\n",
       " 'similac',\n",
       " 'regular',\n",
       " 'went',\n",
       " 'sensit',\n",
       " 'final',\n",
       " 'trick',\n",
       " 'past',\n",
       " 'recal',\n",
       " 'wife',\n",
       " 'becam',\n",
       " 'disgust',\n",
       " 'suggest',\n",
       " 'tri',\n",
       " 'new',\n",
       " 'brand',\n",
       " 'anyway',\n",
       " 'remind',\n",
       " 'babi',\n",
       " 'get',\n",
       " 'older',\n",
       " 'becom',\n",
       " 'fussi',\n",
       " 'mayb',\n",
       " 'dont',\n",
       " 'need',\n",
       " 'anymor',\n",
       " 'that',\n",
       " 'local',\n",
       " 'whole',\n",
       " 'food',\n",
       " 'store',\n",
       " 'bought',\n",
       " 'instant',\n",
       " 'hit',\n",
       " 'son',\n",
       " 'love',\n",
       " 'serious',\n",
       " 'gobbl',\n",
       " 'oz',\n",
       " 'shot',\n",
       " 'would',\n",
       " 'take',\n",
       " 'break',\n",
       " 'actual',\n",
       " 'tast',\n",
       " 'milk',\n",
       " 'said',\n",
       " 'delici',\n",
       " 'compar',\n",
       " 'adult',\n",
       " 'like',\n",
       " 'sure',\n",
       " 'happi',\n",
       " 'thu',\n",
       " 'far',\n",
       " 'go',\n",
       " 'read',\n",
       " 'way',\n",
       " 'extract',\n",
       " 'ideal',\n",
       " 'glad',\n",
       " 'realiz',\n",
       " 'good',\n",
       " 'alway',\n",
       " 'thought',\n",
       " 'stuck',\n",
       " 'fashion',\n",
       " 'parent',\n",
       " 'case',\n",
       " 'sinc',\n",
       " 'caus',\n",
       " 'hormon',\n",
       " 'thank',\n",
       " 'make',\n",
       " 'excel',\n",
       " 'daughter',\n",
       " 'allerg',\n",
       " 'someth',\n",
       " 'breastmilk',\n",
       " 'put',\n",
       " 'work',\n",
       " 'wonder',\n",
       " 'therefor',\n",
       " 'care',\n",
       " 'came',\n",
       " 'time',\n",
       " 'pick',\n",
       " 'supplement',\n",
       " 'stomach',\n",
       " 'react',\n",
       " 'accident',\n",
       " 'dairi',\n",
       " 'toler',\n",
       " 'recommend',\n",
       " 'especi',\n",
       " 'given',\n",
       " 'asian',\n",
       " 'clean',\n",
       " 'label',\n",
       " 'possibl',\n",
       " 'fulfil',\n",
       " 'didnt',\n",
       " 'switch',\n",
       " 'easili',\n",
       " 'realli',\n",
       " 'wish',\n",
       " 'amazon',\n",
       " 'stock',\n",
       " 'pasta',\n",
       " 'seem',\n",
       " 'abl',\n",
       " 'digest',\n",
       " 'easi',\n",
       " 'ad',\n",
       " 'bonu',\n",
       " 'healthi',\n",
       " 'low',\n",
       " 'calori',\n",
       " 'everi',\n",
       " 'version',\n",
       " 'amaz',\n",
       " 'complain',\n",
       " 'firm',\n",
       " 'simpli',\n",
       " 'cook',\n",
       " 'bit',\n",
       " 'longer',\n",
       " 'light',\n",
       " 'think',\n",
       " 'eat',\n",
       " 'homemad',\n",
       " 'almost',\n",
       " 'hard',\n",
       " 'stuff',\n",
       " 'come',\n",
       " 'box',\n",
       " 'day',\n",
       " 'without',\n",
       " 'guilt',\n",
       " 'maintain',\n",
       " 'desir',\n",
       " 'weight',\n",
       " 'problem',\n",
       " 'normal',\n",
       " 'usual',\n",
       " 'mix',\n",
       " 'lean',\n",
       " 'protein',\n",
       " 'meal',\n",
       " 'husband',\n",
       " 'leftov',\n",
       " 'rare',\n",
       " 'howev',\n",
       " 'half',\n",
       " 'bc',\n",
       " 'felt',\n",
       " 'full',\n",
       " 'couldnt',\n",
       " 'high',\n",
       " 'fiber',\n",
       " 'whatev',\n",
       " 'morn',\n",
       " 'hubbi',\n",
       " 'tell',\n",
       " 'he',\n",
       " 'lb',\n",
       " 'im',\n",
       " 'ive',\n",
       " 'week',\n",
       " 'feel',\n",
       " 'last',\n",
       " 'night',\n",
       " 'strong',\n",
       " 'urg',\n",
       " 'snack',\n",
       " 'ate',\n",
       " 'appl',\n",
       " 'habit',\n",
       " 'truli',\n",
       " 'hungri',\n",
       " 'wont',\n",
       " 'say',\n",
       " 'everyon',\n",
       " 'id',\n",
       " 'share',\n",
       " 'bean',\n",
       " 'soup',\n",
       " 'eventu',\n",
       " 'differ',\n",
       " 'varieti',\n",
       " 'shape',\n",
       " 'arriv',\n",
       " 'condit',\n",
       " 'enjoy',\n",
       " 'quick',\n",
       " 'note',\n",
       " 'friend',\n",
       " 'mine',\n",
       " 'issu',\n",
       " 'turn',\n",
       " 'impress',\n",
       " 'content',\n",
       " 'amazingli',\n",
       " 'cheaper',\n",
       " 'prime',\n",
       " 'custom',\n",
       " 'save',\n",
       " 'shippingbr',\n",
       " 'ill',\n",
       " 'back',\n",
       " 'soon',\n",
       " 'qualiti',\n",
       " 'bargain',\n",
       " 'lower',\n",
       " 'carb',\n",
       " 'diet',\n",
       " 'two',\n",
       " 'year',\n",
       " 'miss',\n",
       " 'weve',\n",
       " 'werent',\n",
       " 'crazi',\n",
       " 'gourmet',\n",
       " 'white',\n",
       " 'wheat',\n",
       " 'unlik',\n",
       " 'experi',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'term',\n",
       " 'know',\n",
       " 'mean',\n",
       " 'wait',\n",
       " 'could',\n",
       " 'wow',\n",
       " 'real',\n",
       " 'thing',\n",
       " 'fill',\n",
       " 'keep',\n",
       " 'even',\n",
       " 'miracl',\n",
       " 'children',\n",
       " 'absolut',\n",
       " 'eater',\n",
       " 'world',\n",
       " 'substitut',\n",
       " 'unhealthi',\n",
       " 'correct',\n",
       " 'minut',\n",
       " 'comment',\n",
       " 'web',\n",
       " 'site',\n",
       " 'doubl',\n",
       " 'price',\n",
       " 'write',\n",
       " 'lost',\n",
       " 'pound',\n",
       " 'tire',\n",
       " 'meat',\n",
       " 'nervou',\n",
       " 'splurg',\n",
       " 'across',\n",
       " 'direct',\n",
       " 'top',\n",
       " 'fat',\n",
       " 'greek',\n",
       " 'yogurt',\n",
       " 'chees',\n",
       " 'serv',\n",
       " 'yummi',\n",
       " 'definit',\n",
       " 'cure',\n",
       " 'crave',\n",
       " 'subscrib',\n",
       " 'true',\n",
       " 'dr',\n",
       " 'brought',\n",
       " 'visit',\n",
       " 'rais',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'still',\n",
       " 'expect',\n",
       " 'level',\n",
       " 'pod',\n",
       " 'deliv',\n",
       " 'door',\n",
       " 'auto',\n",
       " 'deliveri',\n",
       " 'cheapest',\n",
       " 'senseo',\n",
       " 'check',\n",
       " 'coffe',\n",
       " 'per',\n",
       " 'vs',\n",
       " 'particular',\n",
       " 'guess',\n",
       " 'mention',\n",
       " 'flavor',\n",
       " 'order',\n",
       " 'three',\n",
       " 'cannot',\n",
       " 'favor',\n",
       " 'agre',\n",
       " 'posit',\n",
       " 'written',\n",
       " 'stress',\n",
       " 'enough',\n",
       " 'bad',\n",
       " 'doesnt',\n",
       " 'describ',\n",
       " 'cup',\n",
       " 'return',\n",
       " 'complet',\n",
       " 'wast',\n",
       " 'money',\n",
       " 'garbag',\n",
       " 'gram',\n",
       " 'stronger',\n",
       " 'melitta',\n",
       " 'individu',\n",
       " 'packag',\n",
       " 'stay',\n",
       " 'fresh',\n",
       " 'dispens',\n",
       " 'nice',\n",
       " 'understand',\n",
       " 'decaf',\n",
       " 'valu',\n",
       " 'beat',\n",
       " 'though',\n",
       " 'stick',\n",
       " 'disappoint',\n",
       " 'highli',\n",
       " 'yum',\n",
       " 'long',\n",
       " 'receiv',\n",
       " 'coupl',\n",
       " 'suspect',\n",
       " 'never',\n",
       " 'aw',\n",
       " 'big',\n",
       " 'vanilla',\n",
       " 'overpow',\n",
       " 'strength',\n",
       " 'packet',\n",
       " 'maker',\n",
       " 'plu',\n",
       " 'decaffein',\n",
       " 'count',\n",
       " 'beach',\n",
       " 'person',\n",
       " 'although',\n",
       " 'rich',\n",
       " 'importantli',\n",
       " 'leav',\n",
       " 'drink',\n",
       " 'tall',\n",
       " 'type',\n",
       " 'cost',\n",
       " 'measur',\n",
       " 'reduc',\n",
       " 'brewer',\n",
       " 'confid',\n",
       " 'choos',\n",
       " 'french',\n",
       " 'decid',\n",
       " 'pud',\n",
       " 'see',\n",
       " 'shelv',\n",
       " 'buy',\n",
       " 'chocol',\n",
       " 'stove',\n",
       " 'equal',\n",
       " 'everywher',\n",
       " 'mother',\n",
       " 'made',\n",
       " 'pie',\n",
       " 'tradit',\n",
       " 'continu',\n",
       " 'sent',\n",
       " 'told',\n",
       " 'send',\n",
       " 'pictur',\n",
       " 'itbr',\n",
       " 'mebr',\n",
       " 'anyth',\n",
       " 'els',\n",
       " 'youbr',\n",
       " 'honey',\n",
       " 'seriou',\n",
       " 'sweet',\n",
       " 'tooth',\n",
       " 'prepar',\n",
       " 'dessert',\n",
       " 'muffin',\n",
       " 'conveni',\n",
       " 'requir',\n",
       " 'toss',\n",
       " 'cuisinart',\n",
       " 'six',\n",
       " 'beauti',\n",
       " 'virtual',\n",
       " 'moist',\n",
       " 'econom',\n",
       " 'follow',\n",
       " 'instruct',\n",
       " 'dri',\n",
       " 'slowli',\n",
       " 'togeth',\n",
       " 'pour',\n",
       " 'spray',\n",
       " 'pan',\n",
       " 'small',\n",
       " 'oven',\n",
       " 'insid',\n",
       " 'smell',\n",
       " 'date',\n",
       " 'pouch',\n",
       " 'tough',\n",
       " 'plastic',\n",
       " 'open',\n",
       " 'bake',\n",
       " 'nd',\n",
       " 'pop',\n",
       " 'min',\n",
       " 'warm',\n",
       " 'complaint',\n",
       " 'difficult',\n",
       " 'worth',\n",
       " 'penni',\n",
       " 'peopl',\n",
       " 'purchas',\n",
       " 'futur',\n",
       " 'amazoncom',\n",
       " 'everyth',\n",
       " 'sun',\n",
       " 'sale',\n",
       " 'fudg',\n",
       " 'chip',\n",
       " 'cooki',\n",
       " 'recip',\n",
       " 'worst',\n",
       " 'ever',\n",
       " 'either',\n",
       " 'someon',\n",
       " 'sens',\n",
       " 'bud',\n",
       " 'taken',\n",
       " 'perman',\n",
       " 'vacat',\n",
       " 'websit',\n",
       " 'kitchen',\n",
       " 'within',\n",
       " 'second',\n",
       " 'threw',\n",
       " 'bottl',\n",
       " 'carri',\n",
       " 'pocket',\n",
       " 'home',\n",
       " 'fell',\n",
       " 'drop',\n",
       " 'ton',\n",
       " 'hot',\n",
       " 'decent',\n",
       " 'ship',\n",
       " 'later',\n",
       " 'item',\n",
       " 'prepackag',\n",
       " 'your',\n",
       " 'look',\n",
       " 'authent',\n",
       " 'thai',\n",
       " 'give',\n",
       " 'name',\n",
       " 'quit',\n",
       " 'accur',\n",
       " 'carton',\n",
       " 'fanci',\n",
       " 'hold',\n",
       " 'grab',\n",
       " 'tasti',\n",
       " 'supermarket',\n",
       " 'guarante',\n",
       " 'tastier',\n",
       " 'option',\n",
       " 'substanc',\n",
       " 'ie',\n",
       " 'noodl',\n",
       " 'talk',\n",
       " 'american',\n",
       " 'overal',\n",
       " 'palat',\n",
       " 'probabl',\n",
       " 'opt',\n",
       " 'next',\n",
       " 'larg',\n",
       " 'straight',\n",
       " 'artifici',\n",
       " 'finish',\n",
       " 'fourth',\n",
       " 'ginger',\n",
       " 'sauc',\n",
       " 'extra',\n",
       " 'star',\n",
       " 'punch',\n",
       " 'may',\n",
       " 'egg',\n",
       " 'tip',\n",
       " 'coconut',\n",
       " 'microwav',\n",
       " 'contain',\n",
       " 'lunch',\n",
       " 'famili',\n",
       " 'dish',\n",
       " 'kid',\n",
       " 'opportun',\n",
       " 'thesebr',\n",
       " 'neg',\n",
       " 'separ',\n",
       " 'season',\n",
       " 'singl',\n",
       " 'combin',\n",
       " 'spice',\n",
       " 'easier',\n",
       " 'cours',\n",
       " 'benefit',\n",
       " 'multipl',\n",
       " 'abil',\n",
       " 'certain',\n",
       " 'fond',\n",
       " 'peanut',\n",
       " 'rock',\n",
       " 'noth',\n",
       " 'lover',\n",
       " 'island',\n",
       " 'right',\n",
       " 'outsid',\n",
       " 'provid',\n",
       " 'water',\n",
       " 'main',\n",
       " 'superior',\n",
       " 'part',\n",
       " 'gluten',\n",
       " 'except',\n",
       " 'shrimp',\n",
       " 'flavorbr',\n",
       " 'remark',\n",
       " 'allow',\n",
       " 'chanc',\n",
       " 'subtl',\n",
       " 'seek',\n",
       " 'pronounc',\n",
       " 'salad',\n",
       " 'dress',\n",
       " 'spici',\n",
       " 'quantiti',\n",
       " 'deal',\n",
       " 'speak',\n",
       " 'walmart',\n",
       " 'cent',\n",
       " 'subscript',\n",
       " 'least',\n",
       " 'near',\n",
       " 'havent',\n",
       " 'yet',\n",
       " 'youll',\n",
       " 'treat',\n",
       " 'mild',\n",
       " 'portabl',\n",
       " 'favorit',\n",
       " 'add',\n",
       " 'anoth',\n",
       " 'chicken',\n",
       " 'ago',\n",
       " 'often',\n",
       " 'glutenfre',\n",
       " 'sick',\n",
       " 'warn',\n",
       " 'help',\n",
       " 'end',\n",
       " 'away',\n",
       " 'thrill',\n",
       " 'nutrit',\n",
       " 'inform',\n",
       " 'quickli',\n",
       " 'here',\n",
       " 'wasnt',\n",
       " 'reward',\n",
       " 'reorder',\n",
       " 'recent',\n",
       " 'similar',\n",
       " 'trader',\n",
       " 'joe',\n",
       " 'present',\n",
       " 'variat',\n",
       " 'starter',\n",
       " 'throw',\n",
       " 'previou',\n",
       " 'theyr',\n",
       " 'kind',\n",
       " 'variou',\n",
       " 'line',\n",
       " 'stir',\n",
       " 'involv',\n",
       " 'done',\n",
       " 'textur',\n",
       " 'clump',\n",
       " 'consid',\n",
       " 'appar',\n",
       " 'cowork',\n",
       " 'ask',\n",
       " 'batch',\n",
       " 'plan',\n",
       " 'edibl',\n",
       " 'mess',\n",
       " 'resembl',\n",
       " 'discern',\n",
       " 'thembr',\n",
       " 'mislead',\n",
       " 'weigh',\n",
       " 'figur',\n",
       " 'otherwis',\n",
       " 'excess',\n",
       " 'might',\n",
       " 'ok',\n",
       " 'melt',\n",
       " 'entir',\n",
       " 'can',\n",
       " 'veggi',\n",
       " 'strip',\n",
       " 'bag',\n",
       " 'frozen',\n",
       " 'restaur',\n",
       " 'curri',\n",
       " 'pretti',\n",
       " 'fast',\n",
       " 'scare',\n",
       " 'crush',\n",
       " 'red',\n",
       " 'pepper',\n",
       " 'control',\n",
       " 'creami',\n",
       " 'green',\n",
       " 'sweeter',\n",
       " 'hotter',\n",
       " 'chang',\n",
       " 'jar',\n",
       " 'pleas',\n",
       " 'hint',\n",
       " 'heat',\n",
       " 'salti',\n",
       " 'fair',\n",
       " 'sodium',\n",
       " 'scratch',\n",
       " 'beef',\n",
       " 'pork',\n",
       " 'tofu',\n",
       " 'sampl',\n",
       " 'other',\n",
       " 'coffeebr',\n",
       " 'total',\n",
       " 'opinion',\n",
       " 'match',\n",
       " 'groceri',\n",
       " 'hotel',\n",
       " 'tea',\n",
       " 'meet',\n",
       " 'fantast',\n",
       " 'compani',\n",
       " 'unabl',\n",
       " 'secur',\n",
       " 'brew',\n",
       " 'color',\n",
       " 'aromat',\n",
       " 'third',\n",
       " 'canist',\n",
       " 'offic',\n",
       " 'sold',\n",
       " 'tart',\n",
       " 'unusu',\n",
       " 'huge',\n",
       " 'hibiscu',\n",
       " 'steep',\n",
       " 'cranberri',\n",
       " 'cold',\n",
       " 'resort',\n",
       " 'herbal',\n",
       " 'relax',\n",
       " 'clear',\n",
       " 'sinu',\n",
       " 'sooth',\n",
       " 'deliciousbr',\n",
       " 'gift',\n",
       " 'afford',\n",
       " 'tobr',\n",
       " 'sweeten',\n",
       " 'must',\n",
       " 'household',\n",
       " 'drinker',\n",
       " 'produc',\n",
       " 'whether',\n",
       " 'north',\n",
       " 'mapl',\n",
       " 'pay',\n",
       " 'sell',\n",
       " 'insan',\n",
       " 'rate',\n",
       " 'higher',\n",
       " 'hope',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'rose',\n",
       " 'black',\n",
       " 'unfortun',\n",
       " 'blend',\n",
       " 'lack',\n",
       " 'bodi',\n",
       " 'caffein',\n",
       " 'fullbodi',\n",
       " 'pressur',\n",
       " 'power',\n",
       " 'doggi',\n",
       " 'newman',\n",
       " 'dog',\n",
       " 'pet',\n",
       " 'budget',\n",
       " 'harmoni',\n",
       " 'farm',\n",
       " 'compromis',\n",
       " 'worri',\n",
       " 'hell',\n",
       " 'trash',\n",
       " 'shell',\n",
       " 'bone',\n",
       " 'there',\n",
       " 'accid',\n",
       " 'shini',\n",
       " 'coat',\n",
       " 'energi',\n",
       " 'bright',\n",
       " 'eye',\n",
       " 'addit',\n",
       " 'vet',\n",
       " 'bill',\n",
       " 'result',\n",
       " 'pricebr',\n",
       " 'fanat',\n",
       " 'fur',\n",
       " 'human',\n",
       " 'updat',\n",
       " 'notic',\n",
       " 'jump',\n",
       " 'buck',\n",
       " 'rancid',\n",
       " 'oil',\n",
       " 'wouldnt',\n",
       " 'readili',\n",
       " 'brown',\n",
       " 'rice',\n",
       " 'etc',\n",
       " 'pricey',\n",
       " 'texa',\n",
       " 'call',\n",
       " 'servic',\n",
       " 'replac',\n",
       " 'due',\n",
       " 'weather',\n",
       " 'prompt',\n",
       " 'death',\n",
       " 'corn',\n",
       " 'convinc',\n",
       " 'wrong',\n",
       " 'anim',\n",
       " 'die',\n",
       " 'constant',\n",
       " 'ear',\n",
       " 'infect',\n",
       " 'allergi',\n",
       " 'sorri',\n",
       " 'address',\n",
       " 'handi',\n",
       " 'loss',\n",
       " 'seen',\n",
       " 'got',\n",
       " 'jack',\n",
       " 'mellow',\n",
       " 'kinda',\n",
       " 'beg',\n",
       " 'skin',\n",
       " 'smoother',\n",
       " 'heck',\n",
       " 'bowl',\n",
       " 'x',\n",
       " 'begin',\n",
       " 'lose',\n",
       " 'shed',\n",
       " 'exactli',\n",
       " 'feed',\n",
       " 'googl',\n",
       " 'commerci',\n",
       " 'detect',\n",
       " 'sort',\n",
       " 'arent',\n",
       " 'suppos',\n",
       " 'eaten',\n",
       " 'purina',\n",
       " 'lamb',\n",
       " 'young',\n",
       " 'troubl',\n",
       " 'yr',\n",
       " 'nutriti',\n",
       " 'began',\n",
       " 'girl',\n",
       " 'theyv',\n",
       " 'report',\n",
       " 'thin',\n",
       " 'eager',\n",
       " 'iam',\n",
       " 'premium',\n",
       " 'shelf',\n",
       " 'incred',\n",
       " 'fewer',\n",
       " 'trip',\n",
       " 'rescu',\n",
       " 'featur',\n",
       " 'articl',\n",
       " 'cover',\n",
       " 'bene',\n",
       " 'basic',\n",
       " 'junk',\n",
       " 'immedi',\n",
       " 'hunt',\n",
       " 'fit',\n",
       " 'awesom',\n",
       " 'she',\n",
       " 'gravi',\n",
       " 'etcbr',\n",
       " 'blow',\n",
       " 'mind',\n",
       " 'typic',\n",
       " 'ratio',\n",
       " 'filler',\n",
       " 'live',\n",
       " 'life',\n",
       " 'ingest',\n",
       " 'list',\n",
       " 'heavi',\n",
       " 'up',\n",
       " 'base',\n",
       " 'juli',\n",
       " 'bowel',\n",
       " 'diseas',\n",
       " 'previous',\n",
       " 'special',\n",
       " 'royal',\n",
       " 'duck',\n",
       " 'potato',\n",
       " 'thrive',\n",
       " 'wo',\n",
       " 'fight',\n",
       " 'toobr',\n",
       " 'verifi',\n",
       " 'wire',\n",
       " 'daili',\n",
       " 'medic',\n",
       " 'gold',\n",
       " 'develop',\n",
       " 'hair',\n",
       " 'soft',\n",
       " 'kibbl',\n",
       " 'rough',\n",
       " 'softer',\n",
       " 'catch',\n",
       " 'cat',\n",
       " 'today',\n",
       " 'avoid',\n",
       " 'chew',\n",
       " 'healthier',\n",
       " 'god',\n",
       " 'ground',\n",
       " 'flax',\n",
       " 'seed',\n",
       " 'pea',\n",
       " 'carrot',\n",
       " 'blueberri',\n",
       " 'byproduct',\n",
       " 'test',\n",
       " 'health',\n",
       " 'known',\n",
       " 'let',\n",
       " 'thoroughli',\n",
       " 'place',\n",
       " 'apart',\n",
       " 'morebr',\n",
       " 'awar',\n",
       " 'mistak',\n",
       " 'welcom',\n",
       " 'surpris',\n",
       " 'heart',\n",
       " 'hand',\n",
       " 'bell',\n",
       " 'ten',\n",
       " 'holist',\n",
       " 'shock',\n",
       " 'nugget',\n",
       " 'grate',\n",
       " 'head',\n",
       " 'toward',\n",
       " 'current',\n",
       " 'terrier',\n",
       " 'gener',\n",
       " 'popular',\n",
       " 'difficulti',\n",
       " 'elimin',\n",
       " 'point',\n",
       " 'attempt',\n",
       " 'breed',\n",
       " 'expens',\n",
       " 'resolv',\n",
       " 'puppi',\n",
       " 'healthybr',\n",
       " 'happier',\n",
       " 'poor',\n",
       " 'mini',\n",
       " 'size',\n",
       " 'common',\n",
       " 'tend',\n",
       " 'specialti',\n",
       " 'target',\n",
       " 'stop',\n",
       " 'movement',\n",
       " 'picki',\n",
       " 'teeth',\n",
       " 'directli',\n",
       " 'terrif',\n",
       " 'avail',\n",
       " 'area',\n",
       " 'camp',\n",
       " 'golden',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b01d67-3fc5-4dba-92bc-1d58b45e5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds, vocabulary):\n",
    "    vectorized_list = []\n",
    "    for sentence in ds:\n",
    "        sentence_list = np.zeros(len(vocabulary))\n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_list[i] = 1\n",
    "        vectorized_list.append(sentence_list)\n",
    "    vectorized_list_new = np.asarray(vectorized_list, dtype=np.float32)\n",
    "    return vectorized_list_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2224e2-6475-4f1a-af06-9efd59435cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentence = vectorizer(preprocessed_sentence, vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86bb5b5c-83de-413c-beb9-b51b33868a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d3bf594-b6ec-4afe-993f-6fd2b0c56c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd2105f7-c201-4178-a0ed-c61091c282ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9339c2bf-ec74-47a1-8996-4816f878614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vectorized_sentence):\n",
    "    prediction = model.predict(vectorized_sentence)\n",
    "    if prediction == 1:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c35b6e68-3b17-43a0-8fcb-460ecfd7d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_lable = get_prediction(vectorized_sentence)\n",
    "prediction_lable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef956c00-4d19-4818-880a-01b24134da0f",
   "metadata": {},
   "source": [
    "### Final code set needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f4793e-6a4c-4e47-b054-9ea9263e65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5366422d-b76a-4e8a-97fe-87b7cbd493ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83415be7-9ce0-40d8-aa5a-5c97e30631f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c905013-c485-484a-8076-44a9da4a30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/corpora/stopwords/english','r') as file:\n",
    "    sw=file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1856ad6-91d1-42c3-8063-b4967b9f3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('../static/model/vocabulary.txt', header=None)\n",
    "vocab_tokens = vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69664992-b8aa-4a3d-9bf9-8834140d5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b8d7de3-5b70-4916-b164-5bd00945ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    data = pd.DataFrame([sentence], columns=['Text'])\n",
    "    data.loc[ :,\"Text\"] = data[\"Text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    data[\"Text\"]=data[\"Text\"].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x,flags=re.MULTILINE) for x in x.split()))\n",
    "    data[\"Text\"]=data[\"Text\"].apply(remove_punctuations)\n",
    "    data[\"Text\"] = data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "    data[\"Text\"]=data[\"Text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "    return data[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d7df5f5-0d00-4000-a5b5-91841e3a025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds, vocabulary):\n",
    "    vectorized_list = []\n",
    "    for sentence in ds:\n",
    "        sentence_list = np.zeros(len(vocabulary))\n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_list[i] = 1\n",
    "        vectorized_list.append(sentence_list)\n",
    "    vectorized_list_new = np.asarray(vectorized_list, dtype=np.float32)\n",
    "    return vectorized_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bfa83ae-b1fa-46a1-a4d8-f6024b2f6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vectorized_sentence):\n",
    "    prediction = model.predict(vectorized_sentence)\n",
    "    if prediction == 1:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae7186bc-5052-4662-8f4b-e9006fa567f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'great product. i like it'\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "vectorized_sentence = vectorizer(preprocessed_sentence, vocab_tokens)\n",
    "prediction = model.predict(vectorized_sentence)\n",
    "print(prediction)\n",
    "prediction_lable = get_prediction(vectorized_sentence)\n",
    "prediction_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7768bdd-e35c-4dd7-9774-a1071b921d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'bad product, i absolutely hate it.'\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "vectorized_sentence = vectorizer(preprocessed_sentence, vocab_tokens)\n",
    "prediction = model.predict(vectorized_sentence)\n",
    "print(prediction)\n",
    "prediction_lable = get_prediction(vectorized_sentence)\n",
    "prediction_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b22759c-9dae-4689-8120-7814daddb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'not interest, but i like it.'\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "vectorized_sentence = vectorizer(preprocessed_sentence, vocab_tokens)\n",
    "prediction = model.predict(vectorized_sentence)\n",
    "print(prediction)\n",
    "prediction_lable = get_prediction(vectorized_sentence)\n",
    "prediction_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "305dfea9-31c8-4ce5-b2e2-5a90ffc2a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'i like to use it and it makes me lot of happiness.'\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "vectorized_sentence = vectorizer(preprocessed_sentence, vocab_tokens)\n",
    "prediction = model.predict(vectorized_sentence)\n",
    "print(prediction)\n",
    "prediction_lable = get_prediction(vectorized_sentence)\n",
    "prediction_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040843b-a666-4fbf-9d7a-c51af9b5d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
